{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Cleaning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD0kCgp0-OAv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "outputId": "668a3e95-9c2a-4a79-f29c-62816cd42f23"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re,os      \n",
        "import string\n",
        "\n",
        "nltk.download(\"popular\")\n",
        "from nltk import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.tokenize import RegexpTokenizer,word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "stop = stopwords.words('english')\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "porter = PorterStemmer()\n",
        "lemma = nltk.WordNetLemmatizer()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CMxiuJnC6wG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(df):\n",
        "  l=len(df)\n",
        "  for i in range(l):\n",
        "    para=str(df.review[i])\n",
        "    #tokenizing\n",
        "    result = tokenizer.tokenize(para)\n",
        "    #taking only strings\n",
        "    words2 = [item for item in result if item.isalpha()]\n",
        "    #stemming\n",
        "    filtered_sentence1 = [porter.stem(word) for word in words2]\n",
        "    #remove stop words\n",
        "    filtered_sentence1 = [w for w in filtered_sentence1 if not w in stop]\n",
        "    para1=\" \".join(x for x in filtered_sentence1)\n",
        "    #lower sentences\n",
        "    ex = [i.lower() for i in para1]\n",
        "    para2=\"\".join(x for x in ex)\n",
        "    df.review[i]=(para2)\n",
        "  return df"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVQ4JAir-lPh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "4cb57161-1552-4002-9d52-be4f8a5931e8"
      },
      "source": [
        "train = pd.read_table(\"train_file.txt\")\n",
        "f = open('test_file.txt', \"r\")\n",
        "test = pd.DataFrame(f.readlines(),columns=['review'])\n",
        "print(train,test.columns)\n",
        "print(\"length of train file:\"+str(len(train)))\n",
        "print(\"length of test file:\"+str(len(test)))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       sentiment                                             review\n",
            "0              1  One of the other reviewers has mentioned that ...\n",
            "1              1  A wonderful little production. <br /><br />The...\n",
            "2              1  I thought this was a wonderful way to spend ti...\n",
            "3             -1  Basically there's a family where a little boy ...\n",
            "4              1  Petter Mattei's \"Love in the Time of Money\" is...\n",
            "...          ...                                                ...\n",
            "14994          1  *** out of ****<br /><br />Yep! Dressed To Kil...\n",
            "14995         -1  Bobcat Goldthwait should be commended for atte...\n",
            "14996          1  And it's not because since her days on \"Claris...\n",
            "14997         -1  A traveling couple (Horton and Hamilton)stumbl...\n",
            "14998         -1  This film is deeply disappointing. Not only th...\n",
            "\n",
            "[14999 rows x 2 columns] Index(['review'], dtype='object')\n",
            "length of train file:14999\n",
            "length of test file:15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCG-xLexDleI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "3509e5a4-3c47-41d5-ed87-61767c355ac4"
      },
      "source": [
        "\n",
        "train = preprocess(train)\n",
        "test = preprocess(test)\n",
        "print(\"After preprocessing\\n\"+str(len(train)),str(len(test)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "After preprocessing\n",
            "14999 15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOmKEdJ2-lXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.to_csv('processed_train.txt')\n",
        "test.to_csv('processed_test.txt')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5WrcBStFok7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "356705fc-7764-4796-f622-7355876563fc"
      },
      "source": [
        "print(\"After preprocessing\\n\"+str(len(train)),str(len(test)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After preprocessing\n",
            "14999 15000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}